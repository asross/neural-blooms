\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{natbib}
\usepackage{bbm}
\usepackage{amsmath}
\newcommand{\tU}{\tilde{\mathcal{U}}}

\title{Improving Learned Bloom Filters: Losses and Updates}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Andrew Slavin Ross \\
  Harvard SEAS \\
  \texttt{andrew\_ross@g.harvard.edu} \\
  \And
  Gal Kaplun \\
  Harvard SEAS \\
  \texttt{galkaplun@g.harvard.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Background and Related Work}

Much of the recent interest in learned index structures has been inspired by
\citet{kraska2018case}, which makes a compelling case for their potential.
They argue that classical index structures like B-Trees, Hash-maps, and Bloom
filters, which are ignorant of the data distribution, could be improved by the
addition of machine learning components, which can be \emph{optimized} under
data distributions.

For Bloom filters, they identify that the core problem they try to solve is
akin to binary classification, specifically determining whether queries $x$ are
from a set of observed keys $\mathcal{K}$ or a set of non-keys $\mathcal{U}$.
Rather than immediately checking a Bloom filter, they train a model $f(x)$ to
predict membership in $\mathcal{K}$ ($\equiv y$) as a binary classification
problem with a simple cross-entropy log loss, \begin{equation}
  \mathcal{L}_{\mathrm{XE}}(f) \equiv \mathbb{E}_{x,y}\left[ y \log f(x) + (1-y) \log (1-f(x)) \right]
\end{equation}
and assume keys are present whenever its output ($f(x) \in [0,1]
\forall x$) is above a threshold $\tau$.

One important detail for this problem is that, while the key set $\mathcal{K}$
is fixed and known beforehand, they must choose specific negative sets for
training and validation which are subsets of $\mathcal{U}$. The validation set,
$\tU$, is used to tune the threshold $\tau$, which defines an empirical false
positive rate \begin{equation}
  \mathrm{FPR}_\tau \equiv \frac{1}{|\tU|} \sum_{x \in \tU} \mathbbm{1}(f(x) > \tau)
\end{equation}
This model-specific false positive rate is not the false positive rate of the
entire learned bloom filter, however, because we must also consider false
negatives \begin{equation}
  \mathcal{K}_{\tau}^{-} \equiv \{ x \in \mathcal{K} \,|\, f(x) < \tau \}.
\end{equation}
To ensure the overall structure never returns false negatives, entries from
$\mathcal{K}_{\tau}^{-}$ must be inserted into a backup bloom filter $B$ with
false positive rate $\mathrm{FPR}_B$, giving an overall false positive
rate\footnote{We would like to thank Michael Mitzenmacher for identifying this
when reviewing \citet{kraska2018case}.} \begin{equation}
  \mathrm{FPR}_O = \mathrm{FPR}_\tau + (1 - \mathrm{FPR}_\tau)\mathrm{FPR}_B.
\end{equation}
In practice, users can tune $\tau$ and the size of the backup filter $B$ to
achieve a desired overall false positive rate.

For their problem, they consider text string data from a URL blacklisting
application, so $\mathcal{K}$ is a set of blacklisted phishing URLs. For their
negative training set and validation set $\tU$, they choose an even mixture of
random valid URLs and random URLs from a whitelist of URLs which are not
phishing pages but have been specifically marked as easily ``mistaken for
phishing pages.'' For the model class of $f$, they choose relatively small GRU
RNNs \citep{cho2014learning}. Overall, they demonstrate moderate reductions in
memory footprints.

The short section on Bloom filters from \citet{kraska2018case} is further
formalized, clarified, and discussed by \citet{mitzenmacher2018model}. Several
points are worth restating here. First, \citet{mitzenmacher2018model} notes
that the claimed advantage of learned bloom filters---their awareness of the
data distribution---can also be seen as limitation. Assuming perfect hashing,
standard Bloom filters have false positive rates that are completely
independent of the data distribution. This means that shifts in the data
distribution (e.g. due to changes in the generating process, or the presence of
an adversary with knowledge of individual queries but not the underlying hash
functions) can never harm performance. However, the performance of learned
Bloom filters can degrade significantly in the presence of such shifts.
In the absence of shifts, however, \citet{mitzenmacher2018model} show that
the difference between a learned Bloom filter's empirical false positive rate
$X$ on a validation set $\mathcal{T}$ and $Y$, its false positive rate on some
future query set $\mathcal{Q}$ (which is the quantity that actually affects
downstream performance), is bounded by \begin{equation}
  \Pr(|X-Y| \leq \epsilon) \leq 2e^{-\epsilon^2|\mathcal{T}|/2} + 2e^{-\epsilon^2|\mathcal{Q}|/2}.
\end{equation}

Another issue, related to distributional shifting, is that updates are more
problematic for learned Bloom filters. Although the backup Bloom filter can
simply receive the update (assuming $f(x') < \tau$ for the new key $x'$, i.e.
$x'$ was not previously a false positive), this tends to come at a higher FPR
increase. Additionally, after a certain number of inputs, users may wish to
reconstruct the learned Bloom filter. Although in both the learned and standard
Bloom filter cases this requires access to $\mathcal{K}$, the learned Bloom
filter also needs $\mathcal{U}$ (which ideally should be expanded to include
previous false positives) and must undergo a retraining procedure that is
likely more computationally intensive than the $k$ hash function calls per
element required for a standard Bloom filter.

\begin{itemize}
    \item \cite{rae2019meta}
    \item \cite{hadian2019considerations}
    \item \cite{mitzenmacher2018model}
    \item \cite{mitzenmacher2018model2}
    \item \cite{liangempirical}
\end{itemize}

\section{Methods}

\section{Results}

\section{Discussion}

\section{Conclusion}

\bibliographystyle{named}
\bibliography{bibliography}

\end{document}
