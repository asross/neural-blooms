{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45244,
     "status": "ok",
     "timestamp": 1572985106213,
     "user": {
      "displayName": "Haojie Shi",
      "photoUrl": "",
      "userId": "16491606329062766703"
     },
     "user_tz": 300
    },
    "id": "4dZ5Q3vWVgc2",
    "outputId": "a2925d27-a311-4eed-b75a-d377d2257e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23235,
     "status": "ok",
     "timestamp": 1572985132475,
     "user": {
      "displayName": "Haojie Shi",
      "photoUrl": "",
      "userId": "16491606329062766703"
     },
     "user_tz": 300
    },
    "id": "Qig2qEGBV8SQ",
    "outputId": "ea977e7b-d4c2-4e92-aabf-34d6c95dc20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitarray\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/2a/35d3bd5bffa9e179267318057a12adc41f837310edf043d8e6d939719f95/bitarray-1.0.1.tar.gz\n",
      "Building wheels for collected packages: bitarray\n",
      "  Building wheel for bitarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bitarray: filename=bitarray-1.0.1-cp36-cp36m-linux_x86_64.whl size=76940 sha256=22bbbb653a22656decdf7b424657ee2c6496ef4b3c345899316f7f7d7fc155c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/e3/1b/82b4f8f9e4f36c6b3f171b1781d5a7d85e74a113f1ee58f0f4\n",
      "Successfully built bitarray\n",
      "Installing collected packages: bitarray\n",
      "Successfully installed bitarray-1.0.1\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.0)\n",
      "Collecting mmh3\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/7e/3ddcab0a9fcea034212c02eb411433db9330e34d626360b97333368b4052/mmh3-2.5.1.tar.gz\n",
      "Building wheels for collected packages: mmh3\n",
      "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mmh3: filename=mmh3-2.5.1-cp36-cp36m-linux_x86_64.whl size=37866 sha256=e8502774bc6ed1094be9701853ceca8a2fb9419ec647d38dafc5109775e5e9e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/b4/ea/6e4e321c625d3320c0c496bf4088371546d8fce5f1dd71b219\n",
      "Successfully built mmh3\n",
      "Installing collected packages: mmh3\n",
      "Successfully installed mmh3-2.5.1\n",
      "Collecting keras==2.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl (319kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (1.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (1.17.3)\n",
      "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.2.5\n",
      "    Uninstalling Keras-2.2.5:\n",
      "      Successfully uninstalled Keras-2.2.5\n",
      "Successfully installed keras-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install bitarray\n",
    "!pip install keras\n",
    "!pip install scikit-learn\n",
    "!pip install mmh3\n",
    "!pip install keras==2.1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14276,
     "status": "ok",
     "timestamp": 1572985140609,
     "user": {
      "displayName": "Haojie Shi",
      "photoUrl": "",
      "userId": "16491606329062766703"
     },
     "user_tz": 300
    },
    "id": "BpeJ9hoSWDBW",
    "outputId": "bbb37c2e-f5d7-405c-bacb-ec0cc70e246c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491178 1435527\n",
      "Baseline\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/DeepBloom-master/lib')\n",
    "sys.path.append('/content/drive/My Drive/DeepBloom-master')\n",
    "#from GRUModel import GRUModel\n",
    "#from DeepBloom import DeepBloom\n",
    "#from DeeperBloom import DeeperBloom\n",
    "import json\n",
    "from utils import *\n",
    "#from test_deep_bloom import test_gru_model\n",
    "#from test_bloom_filter import url_test\n",
    "\n",
    "with open('/content/drive/My Drive/DeepBloom-master/data/dataset.json', 'r') as f:\n",
    "\t    dataset = json.load(f)\n",
    "\n",
    "positives = dataset['positives']\n",
    "negatives = dataset['negatives']\n",
    "\n",
    "positives = positives[:]\n",
    "negatives = negatives[:]\n",
    "print(len(positives), len(negatives))\n",
    "print(\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4998,
     "status": "ok",
     "timestamp": 1572985145624,
     "user": {
      "displayName": "Haojie Shi",
      "photoUrl": "",
      "userId": "16491606329062766703"
     },
     "user_tz": 300
    },
    "id": "X9WdRcLpWFGO",
    "outputId": "ab1dd8a3-3d29-4866-fcc8-5717e940820f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from utils import *\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten\n",
    "from keras.layers import LSTM, Input, GRU\n",
    "from keras import optimizers\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras import losses\n",
    "from keras.engine.topology import Layer\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras import initializers\n",
    "\n",
    "def prelu(x, name='default'):\n",
    "    if name == 'default':\n",
    "        return PReLU(alpha_initializer=initializers.Constant(value=0.25))(x)\n",
    "    else:\n",
    "        return PReLU(alpha_initializer=initializers.Constant(value=0.25), name=name)(x)\n",
    "\n",
    "      \n",
    "def i2str(i):\n",
    "    s = str(i)\n",
    "    if len(s) == 1:\n",
    "        return '0' + s\n",
    "    else:\n",
    "        return s\n",
    "      \n",
    "def focal_loss(alpha_p,alpha_n,gama):\n",
    "\n",
    "    def loss(y_true,y_pred):\n",
    "        y_p = 1-y_pred+y_true*(2*y_pred-1)\n",
    "        alpha = alpha_p*y_true+alpha_n*(1-y_true)\n",
    "        return -K.mean(alpha*K.pow(1-y_p,gama)*K.log(y_p))\n",
    "    return loss\n",
    "\n",
    "def focal_loss_plus1(alpha_p,alpha_n,gama):\n",
    "\n",
    "    def loss(y_true,y_pred):\n",
    "        y_p = 1-y_pred+y_true*(2*y_pred-1)\n",
    "        alpha = alpha_p*y_true+alpha_n*(1-y_true)\n",
    "        k = K.pow(1-y_p,gama)*y_true+(1-y_true)\n",
    "        return -K.mean(alpha*k*K.log(y_p))\n",
    "    return loss\n",
    "\n",
    "def focal_loss_plus2(alpha_p,alpha_n,gama):\n",
    "\n",
    "    def loss(y_true,y_pred):\n",
    "        y_p = 1-y_pred+y_true*(2*y_pred-1)\n",
    "        alpha = alpha_p*y_true+alpha_n*(1-y_true)\n",
    "        k = K.pow(1-y_p,gama)*(1-y_true)+y_true\n",
    "        return -K.mean(alpha*k*K.log(y_p))\n",
    "    return loss\n",
    "\n",
    "def focal_loss_plusS(alpha_p,alpha_n,gama,thr_p,thr_n):\n",
    "\n",
    "    def loss(y_true,y_pred):\n",
    "        y_p = 1-y_pred+y_true*(2*y_pred-1)\n",
    "        alpha = alpha_p*y_true+alpha_n*(1-y_true)\n",
    "        k = K.pow(1-y_p,gama)\n",
    "        y_all = y_true+1-y_true\n",
    "        k_p = K.switch(y_p>thr_p,k,y_all)\n",
    "        k_n = K.switch(y_p>thr_n,k,y_all)\n",
    "        k = k_p*y_true+k_n*(1-y_true)\n",
    "        return -K.mean(alpha*k*K.log(y_p))\n",
    "    return loss\n",
    "\n",
    "def custom_activation(x):\n",
    "    kp = 1\n",
    "    kn = 2\n",
    "    y = K.switch(x>0,kp*x,kn*x)\n",
    "        \n",
    "    #return (K.sigmoid(1/(1+K.exp(K.max(x,0)+K.min(x,0)))))\n",
    "    return (K.sigmoid(y))\n",
    "\n",
    "class CenterLossLayer(Layer):\n",
    "\n",
    "    def __init__(self, alpha=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                       shape=(2, 2),\n",
    "                                       initializer='uniform',\n",
    "                                       trainable=False)\n",
    "        # self.counter = self.add_weight(name='counter',\n",
    "        #                                shape=(1,),\n",
    "        #                                initializer='zeros',\n",
    "        #                                trainable=False)  # just for debugging\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        # x[0] is Nx2, x[1] is Nx10 onehot, self.centers is 10x2\n",
    "        delta_centers = K.dot(K.transpose(x[1]), (K.dot(x[1], self.centers) - x[0]))  # 10x2\n",
    "        center_counts = K.sum(K.transpose(x[1]), axis=1, keepdims=True) + 1  # 10x1\n",
    "        delta_centers /= center_counts\n",
    "        new_centers = self.centers - self.alpha * delta_centers\n",
    "        self.add_update((self.centers, new_centers), x)\n",
    "        weight = np.array([-1,1]).reshape(-1,2)\n",
    "        weight_k = K.variable(value=weight,dtype='float32')\n",
    "        center_dis = K.dot(weight_k,self.centers)\n",
    "        center_dis = K.sum(center_dis ** 2)\n",
    "        # self.add_update((self.counter, self.counter + 1), x)\n",
    "\n",
    "        self.result = x[0] - K.dot(x[1], self.centers)\n",
    "        self.result = K.sum(self.result ** 2, axis=1, keepdims=True) / K.dot(x[1], center_counts)\n",
    "        self.result = self.result/center_dis\n",
    "        return self.result # Nx1\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return 0.5 * K.sum(y_pred, axis=0)\n",
    "\n",
    "def my_visualize_focal(X, y,a,epoch, threshold,ap,an,gama,time,fnr,fpr,lossfn,thr_n,thr_p):\n",
    "    c = ['#ff0000', '#ffff00']\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    X = np.asarray(X).reshape(-1,2)\n",
    "    b = np.where(y==0)[0]\n",
    "    c = np.where(y==1)[0]\n",
    "    # print(b[:10])\n",
    "    d = set(a)&set(c)\n",
    "    d = list(d)\n",
    "    e = set(a)&set(b)\n",
    "    e = list(e)\n",
    "    plt.plot(X[b,0], X[b,1], '.', color = 'red')\n",
    "    plt.plot(X[c,0], X[c,1], '.', color = 'yellow')\n",
    "    plt.plot(X[d,0], X[d,1], '.', color = 'blue')\n",
    "    plt.plot(X[e,0],X[e,1], '.', color = 'green')\n",
    "    legend_elements = [Line2D([0], [0], color=\"white\",markerfacecolor='yellow',marker='o', label='1'),\n",
    "              Line2D([0], [0], color=\"white\",markerfacecolor='red',marker='o', label='0'),\n",
    "              Line2D([0], [0], color=\"white\",markerfacecolor='green',marker='o', label='f'),\n",
    "              Line2D([0], [0], color=\"white\",markerfacecolor='blue',marker='o', label='t')]\n",
    "    plt.title('Thr-{:.3f}'.format(threshold)+'-Ap-{}-An-{}-Gama-{}'.format(ap,an,gama,fnr)+'-FPR-{:.3f}'.format(fpr)+'tp-{}-tn-{}'.format(thr_n,thr_p))\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    plt.savefig('/content/drive/My Drive/DeepBloom-master/data/EX2-'+lossfn+'-epoch-{}-Ap-{}-An-{}-Gama-{}-Time-{}-Tp-{}-Tn-{}-train.png'.format(i2str(epoch), ap,an,gama,time,thr_n,thr_p))\n",
    "    plt.close()\n",
    "\n",
    "def draw_hist(data,title,filepath):\n",
    "  plt.figure()\n",
    "  plt.clf()\n",
    "  plt.hist(data, bins = 20)\n",
    "  plt.title(title)\n",
    "  plt.savefig(filepath)\n",
    "  plt.close()\n",
    "\n",
    "def my_visualize(feat, labels, epoch, lambda_cl):\n",
    "    c = ['#ff0000', '#ffff00']\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    center_x = [0,0]\n",
    "    center_y = [0,0]\n",
    "    for i in range(2):\n",
    "        index = np.where(labels==i)[0]\n",
    "        for ind in index:\n",
    "          plt.plot(feat[ind][0], feat[ind][1], '.', c=c[i])\n",
    "          center_x[i] = feat[ind][0]+center_x[i]\n",
    "          center_y[i] = feat[ind][1]+center_y[i]\n",
    "        center_x[i] = center_x[i]/len(index)\n",
    "        center_y[i] = center_y[i]/len(index)\n",
    "    legend_elements = [Line2D([0], [0], color=\"white\",markerfacecolor=c[0],marker='o', label='0'),\n",
    "                Line2D([0], [0], color=\"white\",markerfacecolor=c[1],marker='o', label='1')]\n",
    "    plt.plot(center_x, center_y, 'kx', mew=2, ms=4)\n",
    "    plt.title('New loss Training data. Lambda_centerloss = {}, Epoch = {}'.format(lambda_cl, epoch))\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    plt.savefig('/content/drive/My Drive/DeepBloom-master/data/newGSD epoch-{}-lambda-{}-train.png'.format(i2str(epoch), lambda_cl))\n",
    "    plt.close()\n",
    "    \n",
    "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
    "class GRUModel(Model):\n",
    "    def __init__(self, embeddings_path, embedding_dim,thr_p=0.5,thr_n=0.5,time=0,lossfn='focal',lambda_centerloss=0.5,alpha_p=1,alpha_n=1,gama=0, lr=0.001, maxlen=50, pca_embedding_dim=None, batch_size=1024, gru_size=16, hidden_size=None, second_gru_size=None, decay=0.0001, epochs=30, lstm=False, dense_only=False,loss_fun = 'binary_crossentropy'):\n",
    "        self.embeddings_path = embeddings_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = lr\n",
    "        self.maxlen = maxlen\n",
    "        self.pca_embedding_dim = pca_embedding_dim\n",
    "        self.model = None\n",
    "        self.batch_size = batch_size\n",
    "        self.gru_size = gru_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.second_gru_size = second_gru_size\n",
    "        self.decay = decay\n",
    "        self.epochs = epochs\n",
    "        self.lstm = lstm\n",
    "        self.dense_only = dense_only\n",
    "        self.loss_fun = loss_fun\n",
    "        self.alpha_p = alpha_p\n",
    "        self.alpha_n = alpha_n\n",
    "        self.gama = gama\n",
    "        self.lambda_centerloss= lambda_centerloss\n",
    "        self.lossfn = lossfn\n",
    "        self.time = time\n",
    "        self.thr_p = thr_p\n",
    "        self.thr_n = thr_n\n",
    "\n",
    "    def fit(self, text_X, text_y):\n",
    "\n",
    "        X, y, self.char_indices, self.indices_char = vectorize_dataset(text_X, text_y, self.maxlen)\n",
    "        num_chars = len(self.char_indices)\n",
    "        print(X.shape)\n",
    "        embedding_vectors = {}\n",
    "        with open(self.embeddings_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line_split = line.strip().split(\" \")\n",
    "                vec = np.array(line_split[1:], dtype=float)\n",
    "                char = line_split[0]\n",
    "                embedding_vectors[char] = vec\n",
    "\n",
    "        embedding_matrix = np.zeros((num_chars + 1, self.embedding_dim))\n",
    "        for char, i in self.char_indices.items():\n",
    "            embedding_vector = embedding_vectors.get(char)\n",
    "            assert(embedding_vector is not None)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        print(embedding_matrix.shape)\n",
    "\n",
    "        if self.pca_embedding_dim:\n",
    "            pca = PCA(n_components=self.pca_embedding_dim)\n",
    "            pca.fit(embedding_matrix[1:])\n",
    "            embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "            embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "            print(\"PCA matrix created\")\n",
    "        \n",
    "            \n",
    "        main_input = Input(shape=(self.maxlen,), name='main_input')\n",
    "        # side_input = Input(shape=(2,),name='side_input')\n",
    "        \n",
    "        x = Embedding(num_chars + 1, self.embedding_dim if not self.pca_embedding_dim else self.pca_embedding_dim, input_length=self.maxlen,\n",
    "    weights=[embedding_matrix] if not self.pca_embedding_dim else [embedding_matrix_pca])(main_input)\n",
    "        x = GRU(self.gru_size, return_sequences=False if not self.second_gru_size else True)(x)\n",
    "        x = Dense(self.hidden_size,activation=prelu, name='hidden_output3',kernel_regularizer=l2(self.decay))(x)\n",
    "        x = Dense(4,activation=prelu, name='hidden_output2',kernel_regularizer=l2(self.decay))(x)\n",
    "        x = Dense(2,activation=prelu, name='hidden_output1',kernel_regularizer=l2(self.decay))(x)\n",
    "        prediction = Dense(1,activation='sigmoid', name='output',kernel_regularizer=l2(self.decay))(x)\n",
    "        # side_output = CenterLossLayer(alpha=0.5, name='centerlosslayer')([x, side_input])\n",
    "        #prediction = Dense(1,activation=custom_activation, name='output')(x)\n",
    "        \n",
    "        self.model = Model(inputs=[main_input],outputs=[prediction])\n",
    "        self.model.summary()\n",
    "        optimizer = optimizers.Adam(lr=self.lr, decay=self.decay)\n",
    "        #optimizer = optimizers.SGD(lr=self.lr, momentum=0.9)\n",
    "        self.model.compile(optimizer=optimizer, loss=focal_loss_plusS(self.alpha_p,self.alpha_n,self.gama,self.thr_p,self.thr_n),metrics=['accuracy'])\n",
    "\n",
    "        y_t = np.zeros((len(y),2),dtype=int)\n",
    "        for j in range(len(y)):\n",
    "          if y[j]:\n",
    "            y_t[j,1] = 1\n",
    "          else:\n",
    "            y_t[j,0] = 1\n",
    "        dummy1 = np.zeros((X.shape[0], 1))\n",
    "        self.model.fit(X,y, batch_size=self.batch_size, epochs=self.epochs, verbose=2)\n",
    "        self.reduced_model = Model(inputs=self.model.input, outputs=self.model.get_layer('hidden_output1').output)\n",
    "        # self.real_model = Model(inputs=self.model.input[0], outputs=self.model.get_layer('output').output)\n",
    "        self.real_model = self.model\n",
    "        #self.model.save(\"/content/drive/My Drive/DeepBloom-master/lib/model_new.h5\")\n",
    "        # self.model = load_model('model.h5')\n",
    "\n",
    "\n",
    "    def predict(self, text_x):\n",
    "        x = np.zeros((1, self.maxlen), dtype=np.int)\n",
    "        offset = max(self.maxlen - len(text_x), 0)\n",
    "        for t, char in enumerate(text_x):\n",
    "            if t >= self.maxlen:\n",
    "                break\n",
    "            x[0, t + offset] = self.char_indices[char]\n",
    "        pred = self.real_model.predict(x)\n",
    "        return pred[0][0]\n",
    "\n",
    "    # Like predict, but you pass in an array of URLs, and it is all\n",
    "    # vectorized in one step, making it more efficient\n",
    "    def predicts(self, text_X):\n",
    "        X = np.zeros((len(text_X), self.maxlen), dtype=np.int)\n",
    "        for i in range(len(text_X)):\n",
    "            offset = max(self.maxlen - len(text_X[i]), 0)\n",
    "            for t, char in enumerate(text_X[i]):\n",
    "                if t >= self.maxlen:\n",
    "                    break\n",
    "                X[i, t + offset] = self.char_indices[char]\n",
    "        preds = [pred[0] for pred in self.real_model.predict(X)]\n",
    "        return preds\n",
    "      \n",
    "    def visualize_train(self,text_X,text_y,threshold,fnr,predictions_f):\n",
    "        text_y = np.asarray(text_y).reshape(-1,1)\n",
    "        # print(text_y[:10])\n",
    "        X = np.zeros((len(text_X), self.maxlen), dtype=np.int)\n",
    "        for i in range(len(text_X)):\n",
    "            offset = max(self.maxlen - len(text_X[i]), 0)\n",
    "            for t, char in enumerate(text_X[i]):\n",
    "                if t >= self.maxlen:\n",
    "                    break\n",
    "                X[i, t + offset] = self.char_indices[char]\n",
    "        preds = [pred for pred in self.reduced_model.predict(X)]\n",
    "        predictions = [pred[0] for pred in self.real_model.predict(X)]\n",
    "        predictions = np.asarray(predictions).reshape(-1,1)\n",
    "        # print(predictions[:10])\n",
    "        a = np.where(predictions>threshold)[0]\n",
    "        b = np.where(text_y==1)[0]\n",
    "        c = list(set(a)&set(b))\n",
    "        fpr = 1-len(c)/len(b)\n",
    "        print('fpr:{}'.format(fpr))\n",
    "        title = 'HNt-'+'ap-{}-an-{}-gama-{}-tp-{}-tn-{}'.format(self.alpha_p,self.alpha_n,self.gama,self.thr_p,self.thr_n)+'-FPR-{:.4f}'.format(fpr)+'-Thr-{:.3f}'.format(threshold)\n",
    "        filepath = '/content/drive/My Drive/DeepBloom-master/data/'+'EX2-Histogram-Negt-loss-'+self.lossfn+'-time-{}-ap-{}-an-{}-gama-{}-Epoch-{}-FNR-{}-Tp-{}-Tn-{}-train.png'.format(self.time,self.alpha_p,self.alpha_n,self.gama,self.epochs, fnr,self.thr_p,self.thr_n)\n",
    "        draw_hist(predictions_f,title,filepath)\n",
    "        title = 'HN-'+'ap-{}-an-{}-gama-{}-tp-{}-tn-{}'.format(self.alpha_p,self.alpha_n,self.gama,self.thr_p,self.thr_n)+'-FPR-{:.4f}'.format(fpr)+'-Thr-{:.3f}'.format(threshold)\n",
    "        filepath = '/content/drive/My Drive/DeepBloom-master/data/'+'EX2-Histogram-Neg-loss-'+self.lossfn+'-time-{}-ap-{}-an-{}-gama-{}-Epoch-{}-FNR-{}-Tp-{}-Tn-{}-train.png'.format(self.time,self.alpha_p,self.alpha_n,self.gama,self.epochs, fnr,self.thr_p,self.thr_n)\n",
    "        draw_hist(predictions[np.where(text_y==0)[0]],title,filepath)\n",
    "        title = 'HP-'+'ap-{}-an-{}-gama-{}-tp-{}-tn-{}'.format(self.alpha_p,self.alpha_n,self.gama,self.thr_p,self.thr_n)+'-FPR-{:.4f}'.format(fpr)+'-Thr-{:.3f}'.format(threshold)\n",
    "        filepath = '/content/drive/My Drive/DeepBloom-master/data/'+'EX2-Histogram-Pos-loss-'+self.lossfn+'-time-{}-ap-{}-an-{}-gama-{}-Epoch-{}-FNR-{}-Tp-{}-Tn-{}-train.png'.format(self.time,self.alpha_p,self.alpha_n,self.gama,self.epochs, fnr,self.thr_p,self.thr_n)\n",
    "        draw_hist(predictions[np.where(text_y==1)[0]],title,filepath)\n",
    "        #my_visualize_focal(preds, text_y,a,self.epochs, threshold,self.alpha_p,self.alpha_n,self.gama,self.time,fnr,fpr,self.lossfn,self.thr_p,self.thr_n)\n",
    "        # my_visualize_new(preds, text_y, epoch=self.epochs,)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4KX2JJ4fE5U"
   },
   "outputs": [],
   "source": [
    "from BloomFilter import BloomFilter\n",
    "import math\n",
    "import random\n",
    "from utils import *\n",
    "import mmh3\n",
    "\n",
    "class DeepBloom(object):\n",
    "    def __init__(self, model, data, fp_rate):\n",
    "        self.model = model\n",
    "        self.threshold = None\n",
    "        self.fp_rate = float(fp_rate)\n",
    "        self.fit(data)\n",
    "        # self.create_bloom_filter(data)\n",
    "\n",
    "    def check(self, item):\n",
    "        if self.model.predict(item) > self.threshold:\n",
    "            return True\n",
    "        return self.bloom_filter.check(item)\n",
    "\n",
    "    def create_bloom_filter(self, data):\n",
    "        print(\"Creating bloom filter\")\n",
    "        false_negatives = []\n",
    "        preds = self.model.predicts(data.positives)\n",
    "        for i in range(len(data.positives)):\n",
    "            if preds[i] <= self.threshold:\n",
    "                false_negatives.append(data.positives[i])\n",
    "        print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "        self.bloom_filter = BloomFilter(\n",
    "            len(false_negatives),\n",
    "            self.fp_rate / 2,\n",
    "            string_digest\n",
    "        )\n",
    "        for fn in false_negatives:\n",
    "            self.bloom_filter.add(fn)\n",
    "        print(\"Created bloom filter\")\n",
    "\n",
    "\n",
    "    def fit(self, data):\n",
    "        ## Split negative data into subgroups.\n",
    "        (s1, s2) = split_negatives(data)\n",
    "        print(\"Training model with train, dev, positives\", len(s1), len(s2), len(data.positives))\n",
    "\n",
    "        ## Shuffle together subset of negatives and positives.\n",
    "        ## Then, train the model on this data.\n",
    "        shuffled = shuffle_for_training(s1, data.positives)\n",
    "        self.model.fit(shuffled[0], shuffled[1])\n",
    "        print(\"Done fitting\")\n",
    "\n",
    "        ## We want a threshold such that at most s2.size * fp_rate/2 elements\n",
    "        ## are greater than threshold.\n",
    "        fp_index = math.ceil((len(s2) * (1 - self.fp_rate/2)))\n",
    "        predictions = self.model.predicts(s2)\n",
    "        predictions.sort()\n",
    "        self.threshold = predictions[fp_index]\n",
    "        self.model.visualize_train(shuffled[0],shuffled[1],self.threshold,self.fp_rate/2,predictions)\n",
    "        print(\"done visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-Gb7DNfl-OW"
   },
   "outputs": [],
   "source": [
    "def test_gru_model(positives, negatives, model, train_dev_fraction=0.9, deeper_bloom=False, fp_rate=0.01, fp_fractions=None):\n",
    "    train_dev_negatives = negatives[:int(train_dev_fraction * len(negatives))]\n",
    "    test_negatives = negatives[int(train_dev_fraction * len(negatives)):]\n",
    "    print(\"Number train, dev\", len(train_dev_negatives))\n",
    "    print(\"Number test\", len(test_negatives))\n",
    "    print(\"Number positives \", len(positives))\n",
    "\n",
    "    data = Data(positives, train_dev_negatives)\n",
    "    if not deeper_bloom:\n",
    "        db = DeepBloom(model, data, fp_rate)\n",
    "        print(\"Params needed\", db.model.model.count_params())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3402801,
     "status": "ok",
     "timestamp": 1572988543924,
     "user": {
      "displayName": "Haojie Shi",
      "photoUrl": "",
      "userId": "16491606329062766703"
     },
     "user_tz": 300
    },
    "id": "sSMUNBo1mGKB",
    "outputId": "4bb9a98e-215c-4de2-de91-4e15709ad416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train, dev 1363750\n",
      "Number test 71777\n",
      "Number positives  1491178\n",
      "Training model with train, dev, positives 1227375 136375 1491178\n",
      "Vectorizing data...\n",
      "Corpus length 51413132\n",
      "['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Total chars: 38\n",
      "[(4, 26), (5, 478), (6, 4385), (7, 8525), (8, 17841), (9, 29160), (10, 49616), (11, 81456), (12, 130824), (13, 167520), (14, 186210), (15, 189876), (16, 191354), (17, 195244), (18, 193089), (19, 182793), (20, 167004), (21, 146920), (22, 127819), (23, 109895), (24, 92890), (25, 78725), (26, 65687), (27, 54689), (28, 46607), (29, 39943), (30, 33373), (31, 28094), (32, 23714), (33, 19983), (34, 11506), (35, 9527), (36, 7046), (37, 5586), (38, 4419), (39, 3443), (40, 2448), (41, 2035), (42, 1530), (43, 1132), (44, 873), (45, 819), (46, 566), (47, 476), (48, 398), (49, 350), (50, 286), (51, 285), (52, 207), (53, 167), (54, 155), (55, 160), (56, 120), (57, 113), (58, 87), (59, 90), (60, 105), (61, 108), (62, 105), (63, 121), (64, 98), (65, 89), (66, 101), (67, 111), (68, 38), (69, 34), (70, 28), (71, 35), (72, 4), (73, 1), (75, 1), (77, 1), (78, 1), (79, 1), (81, 1), (82, 2), (84, 1), (88, 1), (103, 1), (124, 1)]\n",
      "max seen length of URL 124\n",
      "Using maxlen 40\n",
      "(2718553, 40)\n",
      "(39, 50)\n",
      "PCA matrix created\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:171: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:176: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:180: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:189: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:196: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 40, 16)            624       \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 16)                1584      \n",
      "_________________________________________________________________\n",
      "hidden_output3 (Dense)       (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "hidden_output2 (Dense)       (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "hidden_output1 (Dense)       (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 2,393\n",
      "Trainable params: 2,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:744: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2833: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1483: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:960: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/40\n",
      " - 33s - loss: 0.6162 - acc: 0.6247\n",
      "Epoch 2/40\n",
      " - 32s - loss: 0.4529 - acc: 0.7589\n",
      "Epoch 3/40\n",
      " - 32s - loss: 0.4146 - acc: 0.7850\n",
      "Epoch 4/40\n",
      " - 32s - loss: 0.3997 - acc: 0.7957\n",
      "Epoch 5/40\n",
      " - 32s - loss: 0.3912 - acc: 0.8021\n",
      "Epoch 6/40\n",
      " - 32s - loss: 0.3837 - acc: 0.8069\n",
      "Epoch 7/40\n",
      " - 32s - loss: 0.3792 - acc: 0.8103\n",
      "Epoch 8/40\n",
      " - 32s - loss: 0.3775 - acc: 0.8124\n",
      "Epoch 9/40\n",
      " - 32s - loss: 0.3733 - acc: 0.8164\n",
      "Epoch 10/40\n",
      " - 32s - loss: 0.3714 - acc: 0.8176\n",
      "Epoch 11/40\n",
      " - 32s - loss: 0.3696 - acc: 0.8191\n",
      "Epoch 12/40\n",
      " - 32s - loss: 0.3678 - acc: 0.8195\n",
      "Epoch 13/40\n",
      " - 32s - loss: 0.3670 - acc: 0.8208\n",
      "Epoch 14/40\n",
      " - 32s - loss: 0.3646 - acc: 0.8215\n",
      "Epoch 15/40\n",
      " - 32s - loss: 0.3643 - acc: 0.8225\n",
      "Epoch 16/40\n",
      " - 32s - loss: 0.3630 - acc: 0.8234\n",
      "Epoch 17/40\n",
      " - 32s - loss: 0.3621 - acc: 0.8243\n",
      "Epoch 18/40\n",
      " - 32s - loss: 0.3604 - acc: 0.8246\n",
      "Epoch 19/40\n",
      " - 32s - loss: 0.3602 - acc: 0.8250\n",
      "Epoch 20/40\n",
      " - 32s - loss: 0.3598 - acc: 0.8256\n",
      "Epoch 21/40\n",
      " - 32s - loss: 0.3585 - acc: 0.8256\n",
      "Epoch 22/40\n",
      " - 32s - loss: 0.3582 - acc: 0.8261\n",
      "Epoch 23/40\n",
      " - 32s - loss: 0.3575 - acc: 0.8262\n",
      "Epoch 24/40\n",
      " - 32s - loss: 0.3564 - acc: 0.8262\n",
      "Epoch 25/40\n",
      " - 32s - loss: 0.3558 - acc: 0.8262\n",
      "Epoch 26/40\n",
      " - 32s - loss: 0.3554 - acc: 0.8267\n",
      "Epoch 27/40\n",
      " - 32s - loss: 0.3549 - acc: 0.8272\n",
      "Epoch 28/40\n",
      " - 32s - loss: 0.3549 - acc: 0.8266\n",
      "Epoch 29/40\n",
      " - 32s - loss: 0.3541 - acc: 0.8271\n",
      "Epoch 30/40\n",
      " - 32s - loss: 0.3534 - acc: 0.8275\n",
      "Epoch 31/40\n",
      " - 32s - loss: 0.3530 - acc: 0.8277\n",
      "Epoch 32/40\n",
      " - 32s - loss: 0.3525 - acc: 0.8280\n",
      "Epoch 33/40\n",
      " - 32s - loss: 0.3525 - acc: 0.8279\n",
      "Epoch 34/40\n",
      " - 32s - loss: 0.3522 - acc: 0.8281\n",
      "Epoch 35/40\n",
      " - 32s - loss: 0.3520 - acc: 0.8278\n",
      "Epoch 36/40\n",
      " - 33s - loss: 0.3512 - acc: 0.8288\n",
      "Epoch 37/40\n",
      " - 32s - loss: 0.3508 - acc: 0.8289\n",
      "Epoch 38/40\n",
      " - 32s - loss: 0.3507 - acc: 0.8293\n",
      "Epoch 39/40\n",
      " - 32s - loss: 0.3509 - acc: 0.8287\n",
      "Epoch 40/40\n",
      " - 32s - loss: 0.3506 - acc: 0.8291\n",
      "Done fitting\n",
      "fpr:0.38854449301156535\n",
      "done visualization\n",
      "Params needed 2393\n"
     ]
    }
   ],
   "source": [
    "time = 1\n",
    "lossfn = 'focal+S' \n",
    "thr_p = 0.9\n",
    "thr_n = 0.5\n",
    "alpha_p = 1\n",
    "alpha_n = 7\n",
    "gama= 2\n",
    "model = GRUModel('/content/drive/My Drive/DeepBloom-master/data/glove.6B.50d-char.txt', 50,thr_p =thr_p ,thr_n =thr_n ,time=time,lossfn=lossfn,alpha_p = alpha_p,alpha_n =alpha_n,gama=gama, pca_embedding_dim=16, maxlen=40, gru_size=16, batch_size=8192, lr=0.01, hidden_size=8, epochs=40)\n",
    "test_gru_model(positives, negatives, model, train_dev_fraction=0.95, deeper_bloom=False, fp_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3400021,
     "status": "ok",
     "timestamp": 1572991944327,
     "user": {
      "displayName": "Haojie Shi",
      "photoUrl": "",
      "userId": "16491606329062766703"
     },
     "user_tz": 300
    },
    "id": "L1XtWH05TfXO",
    "outputId": "fbc00799-6143-45fc-effb-8f84989c283b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train, dev 1363750\n",
      "Number test 71777\n",
      "Number positives  1491178\n",
      "Training model with train, dev, positives 1227375 136375 1491178\n",
      "Vectorizing data...\n",
      "Corpus length 51413132\n",
      "['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Total chars: 38\n",
      "[(4, 26), (5, 478), (6, 4385), (7, 8525), (8, 17841), (9, 29160), (10, 49616), (11, 81456), (12, 130824), (13, 167520), (14, 186210), (15, 189876), (16, 191354), (17, 195244), (18, 193089), (19, 182793), (20, 167004), (21, 146920), (22, 127819), (23, 109895), (24, 92890), (25, 78725), (26, 65687), (27, 54689), (28, 46607), (29, 39943), (30, 33373), (31, 28094), (32, 23714), (33, 19983), (34, 11506), (35, 9527), (36, 7046), (37, 5586), (38, 4419), (39, 3443), (40, 2448), (41, 2035), (42, 1530), (43, 1132), (44, 873), (45, 819), (46, 566), (47, 476), (48, 398), (49, 350), (50, 286), (51, 285), (52, 207), (53, 167), (54, 155), (55, 160), (56, 120), (57, 113), (58, 87), (59, 90), (60, 105), (61, 108), (62, 105), (63, 121), (64, 98), (65, 89), (66, 101), (67, 111), (68, 38), (69, 34), (70, 28), (71, 35), (72, 4), (73, 1), (75, 1), (77, 1), (78, 1), (79, 1), (81, 1), (82, 2), (84, 1), (88, 1), (103, 1), (124, 1)]\n",
      "max seen length of URL 124\n",
      "Using maxlen 40\n",
      "(2718553, 40)\n",
      "(39, 50)\n",
      "PCA matrix created\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 40, 16)            624       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 16)                1584      \n",
      "_________________________________________________________________\n",
      "hidden_output3 (Dense)       (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "hidden_output2 (Dense)       (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "hidden_output1 (Dense)       (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 2,393\n",
      "Trainable params: 2,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      " - 34s - loss: 0.6200 - acc: 0.6317\n",
      "Epoch 2/40\n",
      " - 32s - loss: 0.4812 - acc: 0.7595\n",
      "Epoch 3/40\n",
      " - 33s - loss: 0.4287 - acc: 0.7885\n",
      "Epoch 4/40\n",
      " - 32s - loss: 0.4091 - acc: 0.7991\n",
      "Epoch 5/40\n",
      " - 33s - loss: 0.4009 - acc: 0.8044\n",
      "Epoch 6/40\n",
      " - 33s - loss: 0.3948 - acc: 0.8067\n",
      "Epoch 7/40\n",
      " - 32s - loss: 0.3891 - acc: 0.8115\n",
      "Epoch 8/40\n",
      " - 32s - loss: 0.3857 - acc: 0.8133\n",
      "Epoch 9/40\n",
      " - 32s - loss: 0.3829 - acc: 0.8153\n",
      "Epoch 10/40\n",
      " - 32s - loss: 0.3807 - acc: 0.8158\n",
      "Epoch 11/40\n",
      " - 32s - loss: 0.3785 - acc: 0.8169\n",
      "Epoch 12/40\n",
      " - 32s - loss: 0.3773 - acc: 0.8182\n",
      "Epoch 13/40\n",
      " - 32s - loss: 0.3756 - acc: 0.8183\n",
      "Epoch 14/40\n",
      " - 32s - loss: 0.3745 - acc: 0.8192\n",
      "Epoch 15/40\n",
      " - 33s - loss: 0.3730 - acc: 0.8192\n",
      "Epoch 16/40\n",
      " - 32s - loss: 0.3726 - acc: 0.8194\n",
      "Epoch 17/40\n",
      " - 32s - loss: 0.3716 - acc: 0.8201\n",
      "Epoch 18/40\n",
      " - 32s - loss: 0.3726 - acc: 0.8199\n",
      "Epoch 19/40\n",
      " - 32s - loss: 0.3699 - acc: 0.8211\n",
      "Epoch 20/40\n",
      " - 32s - loss: 0.3693 - acc: 0.8212\n",
      "Epoch 21/40\n",
      " - 32s - loss: 0.3703 - acc: 0.8218\n",
      "Epoch 22/40\n",
      " - 32s - loss: 0.3682 - acc: 0.8216\n",
      "Epoch 23/40\n",
      " - 33s - loss: 0.3677 - acc: 0.8225\n",
      "Epoch 24/40\n",
      " - 32s - loss: 0.3667 - acc: 0.8222\n",
      "Epoch 25/40\n",
      " - 32s - loss: 0.3661 - acc: 0.8231\n",
      "Epoch 26/40\n",
      " - 32s - loss: 0.3659 - acc: 0.8229\n",
      "Epoch 27/40\n",
      " - 32s - loss: 0.3658 - acc: 0.8238\n",
      "Epoch 28/40\n",
      " - 32s - loss: 0.3642 - acc: 0.8236\n",
      "Epoch 29/40\n",
      " - 32s - loss: 0.3635 - acc: 0.8236\n",
      "Epoch 30/40\n",
      " - 32s - loss: 0.3640 - acc: 0.8241\n",
      "Epoch 31/40\n",
      " - 32s - loss: 0.3624 - acc: 0.8240\n",
      "Epoch 32/40\n",
      " - 32s - loss: 0.3624 - acc: 0.8241\n",
      "Epoch 33/40\n",
      " - 32s - loss: 0.3619 - acc: 0.8244\n",
      "Epoch 34/40\n",
      " - 32s - loss: 0.3613 - acc: 0.8249\n",
      "Epoch 35/40\n",
      " - 32s - loss: 0.3611 - acc: 0.8252\n",
      "Epoch 36/40\n",
      " - 33s - loss: 0.3610 - acc: 0.8247\n",
      "Epoch 37/40\n",
      " - 32s - loss: 0.3605 - acc: 0.8255\n",
      "Epoch 38/40\n",
      " - 32s - loss: 0.3595 - acc: 0.8254\n",
      "Epoch 39/40\n",
      " - 32s - loss: 0.3596 - acc: 0.8260\n",
      "Epoch 40/40\n",
      " - 33s - loss: 0.3595 - acc: 0.8262\n",
      "Done fitting\n",
      "fpr:0.37846051913319534\n",
      "done visualization\n",
      "Params needed 2393\n"
     ]
    }
   ],
   "source": [
    "time = 1\n",
    "lossfn = 'focal+S' \n",
    "thr_p = 0.9\n",
    "thr_n = 0.5\n",
    "alpha_p = 1\n",
    "alpha_n = 8\n",
    "gama= 2\n",
    "model = GRUModel('/content/drive/My Drive/DeepBloom-master/data/glove.6B.50d-char.txt', 50,thr_p =thr_p ,thr_n =thr_n ,time=time,lossfn=lossfn,alpha_p = alpha_p,alpha_n =alpha_n,gama=gama, pca_embedding_dim=16, maxlen=40, gru_size=16, batch_size=8192, lr=0.01, hidden_size=8, epochs=40)\n",
    "test_gru_model(positives, negatives, model, train_dev_fraction=0.95, deeper_bloom=False, fp_rate=0.01)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Focal+.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
